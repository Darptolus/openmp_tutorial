{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1: The offloading model\n",
    "\n",
    "The objective of this lab is to introduce the concept of offloading to accelerator devices. This lab introduces target regions, accelerator devices, and memory management for the device\n",
    "\n",
    "This tutorial is expected to run in a unix-like environment.\n",
    "\n",
    "## Table of content:\n",
    "\n",
    "* Offloading model\n",
    "* Target directive\n",
    "* Memory\n",
    "    * Device memory\n",
    "    * Implicit memory mapping\n",
    "    * Structured memory management\n",
    "    * Unstructured memory management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The offloading model\n",
    "\n",
    "OpenMP supports accelerator devies. These are special devices with compute capabilities that are different to the traditional CPU architectures. The most common accelerator devices is the General Purpuse Graphic Processing Unit (GPGPU or simply GPU).\n",
    "\n",
    "Nvidia devices can be discovered using `nvidia-smi` and amd devices using `rocm-info`. Likewise, compilers like LLVM support special commands to obtain the available accelerator devices. This is the case of `llvm-omp-device-info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 21 20:11:51 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P1000        Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 27%   44C    P0    N/A /  N/A |      0MiB /  4040MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# If you have an NVIDIA device use\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have an AMD device use\n",
    "!rocm-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: llvm-omp-device-info: command not found\n"
     ]
    }
   ],
   "source": [
    "# If you're using llvm use\n",
    "!llvm-omp-device-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target directive\n",
    "The target directive allows a programmer to use accelerator devices, as long as the device is supported by the implementation. For the remaining of this tutorial we will use LLVM with the Clang front end and it will assume that the device is an NVIDIA device. Other compilers that support OpenMP offloading can be found in the [compilers section of the OpenMP website](https://www.openmp.org/resources/openmp-compilers-tools/).\n",
    "\n",
    "## Our first target program\n",
    "One of the simplest target programs that one can write is the following \n",
    "\n",
    "```C\n",
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "\n",
    "int main() {\n",
    "    int a[1] = {0};\n",
    "    #pragma omp target\n",
    "    {\n",
    "        a[0] = omp_is_initial_device();\n",
    "    }\n",
    "    printf(\"Code executed in the %s\\n\",a[0] ? \"Host\":\"Device\");\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang-12: \u001b[0;1;35mwarning: \u001b[0m\u001b[1mUnknown CUDA version. version.txt: 11.0.228. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\u001b[0m\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mNo library 'libomptarget-nvptx-cuda_110-sm_70.bc' found in the default clang lib directory or in LIBRARY_PATH. Please use --libomptarget-nvptx-bc-path to specify nvptx bitcode library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Building this code\n",
    "!clang -fopenmp -fopenmp-targets=nvptx64 C/simple_target.c -o C/simple_target.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the code on the host\n",
    "!OMP_TARGET_OFFLOAD=disabled C/./simple_target.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and play with this code in [simple_multiple_devices](C/simple_target.c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are cheching where the code was executed by using the API function `omp_is_initial_device()`. This API function returns true if the execution environment is the same device that innitiated the target region, i.e. the host. The `omp target` directive does not expose any parallelism per-se. Instead, it tells the compiler that the enclosed region is meant to execute in the device. The compiler will generate different versions of the same code, for different possible devices. Notice that target regions are not necessarily executed on the accelerator device (i.e. GPU in our case). It is still possible to control if target regions are executed in the host as well. This means that this code has a host version and a device version.\n",
    "\n",
    "```\n",
    "Note: An array of only 1 possition is used instead of a scalar to avoid having to keep the code simple. As we will learn later on, arrays have a default mapping of tofrom, while scalars are firstprivate. Allowing to avoid using explicit data mapping. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controling target regions\n",
    "Some important clauses to mention for the target construct are:\n",
    "* `if(condition)`: Allows code to be conditionally executed in the device.\n",
    "* `device(device_num)`: When multiple devices are available, allows to select the different devices\n",
    "* `nowait`: Enables asynchronous execution of code. More on this on a future lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for example the following code\n",
    "\n",
    "```C\n",
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "\n",
    "int main() {\n",
    "    int device_num[omp_get_num_devices()+1] = {0};\n",
    "    int i;\n",
    "    // Iterate over each available device and execute code\n",
    "    // If i == omp_get_num_devices(), execute on the host.\n",
    "    for (i = 0; i <= omp_get_num_devices(); i++) {\n",
    "        #pragma omp target device(i) if(i!=omp_get_num_devices())\n",
    "        {\n",
    "            device_num[0] = omp_get_device_num();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Print which device was used for each region.\n",
    "    for (i = 0; i <= omp_get_num_devices(); i++)\n",
    "        printf(\"Code executed for i = %d in device %d\\n\", i,device_num[i]);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang-12: \u001b[0;1;35mwarning: \u001b[0m\u001b[1mUnknown CUDA version. version.txt: 11.0.228. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\u001b[0m\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mNo library 'libomptarget-nvptx-cuda_110-sm_70.bc' found in the default clang lib directory or in LIBRARY_PATH. Please use --libomptarget-nvptx-bc-path to specify nvptx bitcode library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Building the example\n",
    "!clang -fopenmp -fopenmp-targets=nvptx64 C/simple_multiple_devices.c -o C/simple_multiple_devices.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-82191245d585>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-82191245d585>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    C/./simple_multiple_devices.exe\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Running the example\n",
    "C/./simple_multiple_devices.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and play with this code in [simple_multiple_devices](C/simple_multiple_devices.c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mapping\n",
    "Often case, devices feature a memory and corresponding address space that is independent from the host. While OpenMP supports `unified address space` and `unified shared memory`, learning to manage data between host and device is really important for application performance. OpenMP refers to **\"mapping\"** as the process of moving variables from the host to the device and from the device to the host, allocating variables and deallocating variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example will show that a variable within a target region has a different address value, as long as this code is not executed with unified shared memory support.\n",
    "\n",
    "```C\n",
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "\n",
    "int main() {\n",
    "    int a[1];\n",
    "    printf(\"Addres of a in host = %lx\", (unsigned long)a);\n",
    "    #pragma omp target\n",
    "    {\n",
    "        printf(\"Addres of a in device = %lx\", (unsigned long)a);\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang-12: \u001b[0;1;35mwarning: \u001b[0m\u001b[1mUnknown CUDA version. version.txt: 11.0.228. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\u001b[0m\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mNo library 'libomptarget-nvptx-cuda_110-sm_70.bc' found in the default clang lib directory or in LIBRARY_PATH. Please use --libomptarget-nvptx-bc-path to specify nvptx bitcode library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Building this code\n",
    "!clang -fopenmp -fopenmp-targets=nvptx64 C/different_addresses.c -o C/different_addresses.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: C/./different_addresses.exe: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Running the code\n",
    "!C/./different_addresses.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to play with this code open [different_addresses.c](C/different_addresses.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping can be of the form:\n",
    "* `to`: From host to device.\n",
    "* `from`: From device to host.\n",
    "* `tofrom`: Both from host to device at the beginning of the region, and from device to host at the end of the region.\n",
    "* `alloc`: Only allocate memory, but do not copy values over to the device\n",
    "* `delete`: Used with unstructured data mapping (see below). Delete a variable in the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit data mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far all the example codes we have shown do not use the `map()` clause. Whenever a variable is referenced inside of a target region, and this variable is not in a `map()` clause, it is said that this variable is implicitely mapped.\n",
    "\n",
    "Different variable types have different implicit data mappings. While the complete list of rules can be found in the [specification document](https://www.openmp.org/specifications/), here is a set of rules of thumb that developers should follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar variables\n",
    "Variables that use scalar data types such as `int`, `double`, `float`, etc are mapped as `firstprivate` by default. Therefore, whenever these variables are implicitely mapped, and modified on the devices, they are not copied back to the host. The following example shows this behavior.\n",
    "\n",
    "```C\n",
    "int a = 10;\n",
    "#pragma omp target // implicit firsprivate(a)\n",
    "{\n",
    "    printf(\"a = %d\",a);\n",
    "    a = 20\n",
    "}\n",
    "printf(\"a = %d\",a);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang-12: \u001b[0;1;35mwarning: \u001b[0m\u001b[1mUnknown CUDA version. version.txt: 11.0.228. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\u001b[0m\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mNo library 'libomptarget-nvptx-cuda_110-sm_70.bc' found in the default clang lib directory or in LIBRARY_PATH. Please use --libomptarget-nvptx-bc-path to specify nvptx bitcode library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# building\n",
    "!clang -fopenmp -fopenmp-targets=nvptx64 C/implicit_map_scalar.c -o C/implicit_map_scalar.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: C/./implicit_map_scalar.exe: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Running\n",
    "!C/./implicit_map_scalar.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the file [implicit_map_scalar.c](C/implicit_map_scalar.c) to modify and play with the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non scalar types (classes and structs)\n",
    "User defined types in C and C++ are mapped as `map(tofrom:...)` by default. This means that in stand alone target regions, these variables are copied over to the device at the beginning of the region, and from the device to the host at the end of the region.\n",
    "\n",
    "```\n",
    "Note: Be aware that mapping copies contiguous memory regions. Therefore, no deep copy is performed by default. In order to support deep copy the user must either create a \"declare mapper\" or specify mapping of the different attribute memebers. These cases are ignored for now as it is beyond the purpose of this section\n",
    "```\n",
    "The following is an example of a default mapping of an struvct\n",
    "\n",
    "```C\n",
    "typedef struct myS{\n",
    "    int a;\n",
    "    double *b;\n",
    "}myS_t;\n",
    "\n",
    "int main() {\n",
    "    myS_t myStruct = {1, NULL};\n",
    "\n",
    "    myStruct.b = (double *)malloc(sizeof(double));\n",
    "    myStruct->b = 11.1;\n",
    "\n",
    "    printf(\"Host {%d, %lx}\\n\", myStruct.a, myStruct.b);\n",
    "\n",
    "    #pragma omp target // implicit map(tofrom:myStruct). Not implicit map(tofrom:myStruct.b[0:1])\n",
    "    {\n",
    "        myStruct.a = 10;\n",
    "        // printf(\"%f\\n\", myStruct.b); error since b is not deep copied\n",
    "        printf(\"Device {%d, %lx}\\n\", myStruct.a, myStruct.b);\n",
    "    }\n",
    "\n",
    "    printf(\"Host {%d, %lx}\\n\", myStruct.a, myStruct.b);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang-12: \u001b[0;1;35mwarning: \u001b[0m\u001b[1mUnknown CUDA version. version.txt: 11.0.228. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\u001b[0m\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mNo library 'libomptarget-nvptx-cuda_110-sm_70.bc' found in the default clang lib directory or in LIBRARY_PATH. Please use --libomptarget-nvptx-bc-path to specify nvptx bitcode library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# building\n",
    "!clang -fopenmp -fopenmp-targets=nvptx64 C/implicit_map_struct.c -o C/implicit_map_struct.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: C/./implicit_map_struct.exe: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Running\n",
    "!C/./implicit_map_struct.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with this code in [implicit_map_struct.c](C/implicit_map_struct.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays (not pointers)\n",
    "\n",
    "Arrays for which the compiler can determine the size are also mapped as `map(tofrom:...)`.\n",
    "\n",
    "```C\n",
    "\n",
    "int main() {\n",
    "    int A[] = {1,2,3};\n",
    "\n",
    "    printf(\"Host {%d, %d, %d}\\n\", A[0],A[1],A[2]);\n",
    "\n",
    "    #pragma omp target // implicit map(tofrom:A[0:3])\n",
    "    {\n",
    "        printf(\"Device {%d, %d, %d}\\n\", A[0],A[1],A[2]);\n",
    "        A[0]++; A[1]++; A[2]++;\n",
    "    }\n",
    "\n",
    "    printf(\"Host {%d, %d, %d}\\n\", A[0],A[1],A[2]);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang-12: \u001b[0;1;35mwarning: \u001b[0m\u001b[1mUnknown CUDA version. version.txt: 11.0.228. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\u001b[0m\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mNo library 'libomptarget-nvptx-cuda_110-sm_70.bc' found in the default clang lib directory or in LIBRARY_PATH. Please use --libomptarget-nvptx-bc-path to specify nvptx bitcode library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# building\n",
    "!clang -fopenmp -fopenmp-targets=nvptx64 C/implicit_map_arrays.c -o C/implicit_map_arrays.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: C/./implicit_map_arrays.exe: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Running\n",
    "!C/./implicit_map_arrays.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with this code go to [implicit_map_arrays.c](C/implicit_map_arrays.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers\n",
    "\n",
    "Pointers are a special case. Pointers are also mapped `map(tofrom:...)` by default. However, since it is not possible to determine how many elements are pointed by a pointer, the compiler cannot detemrine the size of the map. Pointers are therefore mapped as `tofrom:ptr[0:0]` where `[0:0]` means, starting from the 0 possition, copy 0 elements. This is confusing at first, but it allows the compiler to perform pointer translation when the array has been previously mapped to the device (e.g. using structured or unstructured data mapping as we will see later on). \n",
    "\n",
    "```C\n",
    "\n",
    "int main() {\n",
    "    int *A = (int*) malloc(3*sizeof(int));\n",
    "\n",
    "    printf(\"Host {%d, %d, %d}\\n\", A[0],A[1],A[2]);\n",
    "\n",
    "    #pragma omp target // implicit map(tofrom:A[0:0])\n",
    "    {\n",
    "        // printf(\"Device {%d, %d, %d}\\n\", A[0],A[1],A[2]); Error since pointer is not mapped. \n",
    "        // A[0]++; A[1]++; A[2]++; Error since pointer is not mapped\n",
    "        printf(\"Cannot access A[]\\n\");\n",
    "    }\n",
    "\n",
    "    #pragma omp target data map(tofrom: A[0:3])\n",
    "    {\n",
    "        #pragma omp target // implicit mapping map(tofrom:A[0:0])\n",
    "        {\n",
    "            // A is automatically translated to a previously mapped location\n",
    "            printf(\"Device {%d, %d, %d}\\n\", A[0],A[1],A[2]); \n",
    "            A[0]++; A[1]++; A[2]++;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    printf(\"Host {%d, %d, %d}\\n\", A[0],A[1],A[2]);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clang-12: \u001b[0;1;35mwarning: \u001b[0m\u001b[1mUnknown CUDA version. version.txt: 11.0.228. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\u001b[0m\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mNo library 'libomptarget-nvptx-cuda_110-sm_70.bc' found in the default clang lib directory or in LIBRARY_PATH. Please use --libomptarget-nvptx-bc-path to specify nvptx bitcode library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# building\n",
    "!clang -fopenmp -fopenmp-targets=nvptx64 C/implicit_map_pointers.c -o C/implicit_map_pointers.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: C/./implicit_map_pointers.exe: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Running\n",
    "!C/./implicit_map_pointers.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with this code go to [implicit_map_pointers.c](C/implicit_map_pointers.c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
